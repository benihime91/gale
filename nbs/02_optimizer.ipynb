{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a8ad11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# skip\n",
    "!git clone https://github.com/benihime91/gale # install gale on colab\n",
    "!pip install -e \"gale[dev]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97eabc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6eb27b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"# hide\\n%load_ext nb_black\\n%load_ext autoreload\\n%autoreload 2\\n%matplotlib inline\";\n",
       "                var nbb_formatted_code = \"# hide\\n%load_ext nb_black\\n%load_ext autoreload\\n%autoreload 2\\n%matplotlib inline\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "%load_ext nb_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b283dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"# hide\\nimport warnings\\nfrom nbdev.export import *\\nfrom nbdev.showdoc import *\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\";\n",
       "                var nbb_formatted_code = \"# hide\\nimport warnings\\nfrom nbdev.export import *\\nfrom nbdev.showdoc import *\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "import warnings\n",
    "from nbdev.export import *\n",
    "from nbdev.showdoc import *\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b0f0e6",
   "metadata": {},
   "source": [
    "# Optimizers\n",
    "> Collection of usefull `Optimizers` their variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1234b694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"# export\\nimport math\\nfrom typing import *\\n\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nfrom fastcore.all import delegates\\nfrom timm.optim import Lookahead, RAdam, RMSpropTF\\nfrom torch.optim import SGD, Adam, AdamW, RMSprop\\nfrom torch.optim.optimizer import Optimizer, required\\n\\nfrom gale.utils.structures import OPTIM_REGISTRY\";\n",
       "                var nbb_formatted_code = \"# export\\nimport math\\nfrom typing import *\\n\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nfrom fastcore.all import delegates\\nfrom timm.optim import Lookahead, RAdam, RMSpropTF\\nfrom torch.optim import SGD, Adam, AdamW, RMSprop\\nfrom torch.optim.optimizer import Optimizer, required\\n\\nfrom gale.utils.structures import OPTIM_REGISTRY\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "import math\n",
    "from typing import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from fastcore.all import delegates\n",
    "from timm.optim import Lookahead, RAdam, RMSpropTF\n",
    "from torch.optim import SGD, Adam, AdamW, RMSprop\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "\n",
    "from gale.utils.structures import OPTIM_REGISTRY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e736f07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"# export\\n_all_ = [\\\"RAdam\\\", \\\"RMSpropTF\\\", \\\"SGD\\\", \\\"Adam\\\", \\\"AdamW\\\", \\\"RMSprop\\\", \\\"OPTIM_REGISTRY\\\"]\";\n",
       "                var nbb_formatted_code = \"# export\\n_all_ = [\\\"RAdam\\\", \\\"RMSpropTF\\\", \\\"SGD\\\", \\\"Adam\\\", \\\"AdamW\\\", \\\"RMSprop\\\", \\\"OPTIM_REGISTRY\\\"]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "_all_ = [\"RAdam\", \"RMSpropTF\", \"SGD\", \"Adam\", \"AdamW\", \"RMSprop\", \"OPTIM_REGISTRY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50965e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"# export\\n_torch_meta = [SGD, Adam, AdamW, RMSprop, RMSpropTF]\\nfor o in _torch_meta:\\n    OPTIM_REGISTRY.register(o)\";\n",
       "                var nbb_formatted_code = \"# export\\n_torch_meta = [SGD, Adam, AdamW, RMSprop, RMSpropTF]\\nfor o in _torch_meta:\\n    OPTIM_REGISTRY.register(o)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "_torch_meta = [SGD, Adam, AdamW, RMSprop, RMSpropTF]\n",
    "for o in _torch_meta:\n",
    "    OPTIM_REGISTRY.register(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415e486a",
   "metadata": {},
   "source": [
    "## LookAhead + RAdam -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e137ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"# export\\n@OPTIM_REGISTRY.register()\\n@delegates(RAdam)\\ndef Ranger(\\n    params: Iterable,\\n    betas: Tuple[float, float] = (0.95, 0.999),\\n    eps: float = 1e-5,\\n    k: int = 6,\\n    alpha: float = 0.5,\\n    **kwargs\\n):\\n    \\\"Convenience method for `Lookahead` with `RAdam`\\\"\\n    return Lookahead(RAdam(params, betas=betas, eps=eps, **kwargs), alpha=alpha, k=k)\";\n",
       "                var nbb_formatted_code = \"# export\\n@OPTIM_REGISTRY.register()\\n@delegates(RAdam)\\ndef Ranger(\\n    params: Iterable,\\n    betas: Tuple[float, float] = (0.95, 0.999),\\n    eps: float = 1e-5,\\n    k: int = 6,\\n    alpha: float = 0.5,\\n    **kwargs\\n):\\n    \\\"Convenience method for `Lookahead` with `RAdam`\\\"\\n    return Lookahead(RAdam(params, betas=betas, eps=eps, **kwargs), alpha=alpha, k=k)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "@OPTIM_REGISTRY.register()\n",
    "@delegates(RAdam)\n",
    "def Ranger(\n",
    "    params: Iterable,\n",
    "    betas: Tuple[float, float] = (0.95, 0.999),\n",
    "    eps: float = 1e-5,\n",
    "    k: int = 6,\n",
    "    alpha: float = 0.5,\n",
    "    **kwargs\n",
    "):\n",
    "    \"Convenience method for `Lookahead` with `RAdam`\"\n",
    "    return Lookahead(RAdam(params, betas=betas, eps=eps, **kwargs), alpha=alpha, k=k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228abf59",
   "metadata": {},
   "source": [
    "## LookAhead + RAdam with Gradient Centralization -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4613d8b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"# export\\n@OPTIM_REGISTRY.register()\\nclass RangerGC(Optimizer):\\n    \\\"\\\"\\\"\\n    Ranger deep learning optimizer - RAdam + Lookahead + Gradient Centralization, combined into one optimizer.\\n\\n    Source - https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer/blob/master/ranger/ranger.py\\n    \\\"\\\"\\\"\\n\\n    def __init__(\\n        self,\\n        params: Iterable,\\n        lr: float = 1e-3,\\n        alpha: float = 0.5,\\n        k: int = 6,\\n        N_sma_threshhold: int = 5,\\n        betas: Tuple[float, float] = (0.95, 0.999),\\n        eps: float = 1e-5,\\n        weight_decay: Union[float, int] = 0,\\n        use_gc: bool = True,\\n        gc_conv_only: bool = False,\\n    ):\\n\\n        # parameter checks\\n        if not 0.0 <= alpha <= 1.0:\\n            raise ValueError(f\\\"Invalid slow update rate: {alpha}\\\")\\n        if not 1 <= k:\\n            raise ValueError(f\\\"Invalid lookahead steps: {k}\\\")\\n        if not lr > 0:\\n            raise ValueError(f\\\"Invalid Learning Rate: {lr}\\\")\\n        if not eps > 0:\\n            raise ValueError(f\\\"Invalid eps: {eps}\\\")\\n\\n        # prep defaults and init torch.optim base\\n        defaults = dict(\\n            lr=lr,\\n            alpha=alpha,\\n            k=k,\\n            step_counter=0,\\n            betas=betas,\\n            N_sma_threshhold=N_sma_threshhold,\\n            eps=eps,\\n            weight_decay=weight_decay,\\n        )\\n        super().__init__(params, defaults)\\n\\n        # adjustable threshold\\n        self.N_sma_threshhold = N_sma_threshhold\\n\\n        # look ahead params\\n\\n        self.alpha = alpha\\n        self.k = k\\n\\n        # radam buffer for state\\n        self.radam_buffer = [[None, None, None] for ind in range(10)]\\n\\n        # gc on or off\\n        self.use_gc = use_gc\\n\\n        # level of gradient centralization\\n        self.gc_gradient_threshold = 3 if gc_conv_only else 1\\n\\n    def __setstate__(self, state):\\n        super(Ranger, self).__setstate__(state)\\n\\n    def step(self, closure=None):\\n        loss = None\\n\\n        if closure is not None:\\n            loss = closure()\\n\\n        # Evaluate averages and grad, update param tensors\\n        for group in self.param_groups:\\n\\n            for p in group[\\\"params\\\"]:\\n                if p.grad is None:\\n                    continue\\n                grad = p.grad.data.float()\\n\\n                if grad.is_sparse:\\n                    raise RuntimeError(\\n                        \\\"Ranger optimizer does not support sparse gradients\\\"\\n                    )\\n\\n                p_data_fp32 = p.data.float()\\n\\n                state = self.state[p]\\n\\n                if len(state) == 0:\\n                    state[\\\"step\\\"] = 0\\n                    state[\\\"exp_avg\\\"] = torch.zeros_like(p_data_fp32)\\n                    state[\\\"exp_avg_sq\\\"] = torch.zeros_like(p_data_fp32)\\n\\n                    # look ahead weight storage now in state dict\\n                    state[\\\"slow_buffer\\\"] = torch.empty_like(p.data)\\n                    state[\\\"slow_buffer\\\"].copy_(p.data)\\n\\n                else:\\n                    state[\\\"exp_avg\\\"] = state[\\\"exp_avg\\\"].type_as(p_data_fp32)\\n                    state[\\\"exp_avg_sq\\\"] = state[\\\"exp_avg_sq\\\"].type_as(p_data_fp32)\\n\\n                # begin computations\\n                exp_avg, exp_avg_sq = state[\\\"exp_avg\\\"], state[\\\"exp_avg_sq\\\"]\\n                beta1, beta2 = group[\\\"betas\\\"]\\n\\n                # GC operation for Conv layers and FC layers\\n                if grad.dim() > self.gc_gradient_threshold:\\n                    grad.add_(-grad.mean(dim=tuple(range(1, grad.dim())), keepdim=True))\\n\\n                state[\\\"step\\\"] += 1\\n\\n                # compute variance mov avg\\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\\n                # compute mean moving avg\\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\\n\\n                buffered = self.radam_buffer[int(state[\\\"step\\\"] % 10)]\\n\\n                if state[\\\"step\\\"] == buffered[0]:\\n                    N_sma, step_size = buffered[1], buffered[2]\\n                else:\\n                    buffered[0] = state[\\\"step\\\"]\\n                    beta2_t = beta2 ** state[\\\"step\\\"]\\n                    N_sma_max = 2 / (1 - beta2) - 1\\n                    N_sma = N_sma_max - 2 * state[\\\"step\\\"] * beta2_t / (1 - beta2_t)\\n                    buffered[1] = N_sma\\n                    if N_sma > self.N_sma_threshhold:\\n                        step_size = math.sqrt(\\n                            (1 - beta2_t)\\n                            * (N_sma - 4)\\n                            / (N_sma_max - 4)\\n                            * (N_sma - 2)\\n                            / N_sma\\n                            * N_sma_max\\n                            / (N_sma_max - 2)\\n                        ) / (1 - beta1 ** state[\\\"step\\\"])\\n                    else:\\n                        step_size = 1.0 / (1 - beta1 ** state[\\\"step\\\"])\\n                    buffered[2] = step_size\\n\\n                if group[\\\"weight_decay\\\"] != 0:\\n                    p_data_fp32.add_(-group[\\\"weight_decay\\\"] * group[\\\"lr\\\"], p_data_fp32)\\n\\n                # apply lr\\n                if N_sma > self.N_sma_threshhold:\\n                    denom = exp_avg_sq.sqrt().add_(group[\\\"eps\\\"])\\n                    p_data_fp32.addcdiv_(-step_size * group[\\\"lr\\\"], exp_avg, denom)\\n                else:\\n                    p_data_fp32.add_(-step_size * group[\\\"lr\\\"], exp_avg)\\n\\n                p.data.copy_(p_data_fp32)\\n\\n                # integrated look ahead...\\n                # we do it at the param level instead of group level\\n                if state[\\\"step\\\"] % group[\\\"k\\\"] == 0:\\n                    # get access to slow param tensor\\n                    slow_p = state[\\\"slow_buffer\\\"]\\n                    # (fast weights - slow weights) * alpha\\n                    slow_p.add_(self.alpha, p.data - slow_p)\\n                    # copy interpolated weights to RAdam param tensor\\n                    p.data.copy_(slow_p)\\n\\n        return loss\";\n",
       "                var nbb_formatted_code = \"# export\\n@OPTIM_REGISTRY.register()\\nclass RangerGC(Optimizer):\\n    \\\"\\\"\\\"\\n    Ranger deep learning optimizer - RAdam + Lookahead + Gradient Centralization, combined into one optimizer.\\n\\n    Source - https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer/blob/master/ranger/ranger.py\\n    \\\"\\\"\\\"\\n\\n    def __init__(\\n        self,\\n        params: Iterable,\\n        lr: float = 1e-3,\\n        alpha: float = 0.5,\\n        k: int = 6,\\n        N_sma_threshhold: int = 5,\\n        betas: Tuple[float, float] = (0.95, 0.999),\\n        eps: float = 1e-5,\\n        weight_decay: Union[float, int] = 0,\\n        use_gc: bool = True,\\n        gc_conv_only: bool = False,\\n    ):\\n\\n        # parameter checks\\n        if not 0.0 <= alpha <= 1.0:\\n            raise ValueError(f\\\"Invalid slow update rate: {alpha}\\\")\\n        if not 1 <= k:\\n            raise ValueError(f\\\"Invalid lookahead steps: {k}\\\")\\n        if not lr > 0:\\n            raise ValueError(f\\\"Invalid Learning Rate: {lr}\\\")\\n        if not eps > 0:\\n            raise ValueError(f\\\"Invalid eps: {eps}\\\")\\n\\n        # prep defaults and init torch.optim base\\n        defaults = dict(\\n            lr=lr,\\n            alpha=alpha,\\n            k=k,\\n            step_counter=0,\\n            betas=betas,\\n            N_sma_threshhold=N_sma_threshhold,\\n            eps=eps,\\n            weight_decay=weight_decay,\\n        )\\n        super().__init__(params, defaults)\\n\\n        # adjustable threshold\\n        self.N_sma_threshhold = N_sma_threshhold\\n\\n        # look ahead params\\n\\n        self.alpha = alpha\\n        self.k = k\\n\\n        # radam buffer for state\\n        self.radam_buffer = [[None, None, None] for ind in range(10)]\\n\\n        # gc on or off\\n        self.use_gc = use_gc\\n\\n        # level of gradient centralization\\n        self.gc_gradient_threshold = 3 if gc_conv_only else 1\\n\\n    def __setstate__(self, state):\\n        super(Ranger, self).__setstate__(state)\\n\\n    def step(self, closure=None):\\n        loss = None\\n\\n        if closure is not None:\\n            loss = closure()\\n\\n        # Evaluate averages and grad, update param tensors\\n        for group in self.param_groups:\\n\\n            for p in group[\\\"params\\\"]:\\n                if p.grad is None:\\n                    continue\\n                grad = p.grad.data.float()\\n\\n                if grad.is_sparse:\\n                    raise RuntimeError(\\n                        \\\"Ranger optimizer does not support sparse gradients\\\"\\n                    )\\n\\n                p_data_fp32 = p.data.float()\\n\\n                state = self.state[p]\\n\\n                if len(state) == 0:\\n                    state[\\\"step\\\"] = 0\\n                    state[\\\"exp_avg\\\"] = torch.zeros_like(p_data_fp32)\\n                    state[\\\"exp_avg_sq\\\"] = torch.zeros_like(p_data_fp32)\\n\\n                    # look ahead weight storage now in state dict\\n                    state[\\\"slow_buffer\\\"] = torch.empty_like(p.data)\\n                    state[\\\"slow_buffer\\\"].copy_(p.data)\\n\\n                else:\\n                    state[\\\"exp_avg\\\"] = state[\\\"exp_avg\\\"].type_as(p_data_fp32)\\n                    state[\\\"exp_avg_sq\\\"] = state[\\\"exp_avg_sq\\\"].type_as(p_data_fp32)\\n\\n                # begin computations\\n                exp_avg, exp_avg_sq = state[\\\"exp_avg\\\"], state[\\\"exp_avg_sq\\\"]\\n                beta1, beta2 = group[\\\"betas\\\"]\\n\\n                # GC operation for Conv layers and FC layers\\n                if grad.dim() > self.gc_gradient_threshold:\\n                    grad.add_(-grad.mean(dim=tuple(range(1, grad.dim())), keepdim=True))\\n\\n                state[\\\"step\\\"] += 1\\n\\n                # compute variance mov avg\\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\\n                # compute mean moving avg\\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\\n\\n                buffered = self.radam_buffer[int(state[\\\"step\\\"] % 10)]\\n\\n                if state[\\\"step\\\"] == buffered[0]:\\n                    N_sma, step_size = buffered[1], buffered[2]\\n                else:\\n                    buffered[0] = state[\\\"step\\\"]\\n                    beta2_t = beta2 ** state[\\\"step\\\"]\\n                    N_sma_max = 2 / (1 - beta2) - 1\\n                    N_sma = N_sma_max - 2 * state[\\\"step\\\"] * beta2_t / (1 - beta2_t)\\n                    buffered[1] = N_sma\\n                    if N_sma > self.N_sma_threshhold:\\n                        step_size = math.sqrt(\\n                            (1 - beta2_t)\\n                            * (N_sma - 4)\\n                            / (N_sma_max - 4)\\n                            * (N_sma - 2)\\n                            / N_sma\\n                            * N_sma_max\\n                            / (N_sma_max - 2)\\n                        ) / (1 - beta1 ** state[\\\"step\\\"])\\n                    else:\\n                        step_size = 1.0 / (1 - beta1 ** state[\\\"step\\\"])\\n                    buffered[2] = step_size\\n\\n                if group[\\\"weight_decay\\\"] != 0:\\n                    p_data_fp32.add_(-group[\\\"weight_decay\\\"] * group[\\\"lr\\\"], p_data_fp32)\\n\\n                # apply lr\\n                if N_sma > self.N_sma_threshhold:\\n                    denom = exp_avg_sq.sqrt().add_(group[\\\"eps\\\"])\\n                    p_data_fp32.addcdiv_(-step_size * group[\\\"lr\\\"], exp_avg, denom)\\n                else:\\n                    p_data_fp32.add_(-step_size * group[\\\"lr\\\"], exp_avg)\\n\\n                p.data.copy_(p_data_fp32)\\n\\n                # integrated look ahead...\\n                # we do it at the param level instead of group level\\n                if state[\\\"step\\\"] % group[\\\"k\\\"] == 0:\\n                    # get access to slow param tensor\\n                    slow_p = state[\\\"slow_buffer\\\"]\\n                    # (fast weights - slow weights) * alpha\\n                    slow_p.add_(self.alpha, p.data - slow_p)\\n                    # copy interpolated weights to RAdam param tensor\\n                    p.data.copy_(slow_p)\\n\\n        return loss\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "@OPTIM_REGISTRY.register()\n",
    "class RangerGC(Optimizer):\n",
    "    \"\"\"\n",
    "    Ranger deep learning optimizer - RAdam + Lookahead + Gradient Centralization, combined into one optimizer.\n",
    "\n",
    "    Source - https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer/blob/master/ranger/ranger.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params: Iterable,\n",
    "        lr: float = 1e-3,\n",
    "        alpha: float = 0.5,\n",
    "        k: int = 6,\n",
    "        N_sma_threshhold: int = 5,\n",
    "        betas: Tuple[float, float] = (0.95, 0.999),\n",
    "        eps: float = 1e-5,\n",
    "        weight_decay: Union[float, int] = 0,\n",
    "        use_gc: bool = True,\n",
    "        gc_conv_only: bool = False,\n",
    "    ):\n",
    "\n",
    "        # parameter checks\n",
    "        if not 0.0 <= alpha <= 1.0:\n",
    "            raise ValueError(f\"Invalid slow update rate: {alpha}\")\n",
    "        if not 1 <= k:\n",
    "            raise ValueError(f\"Invalid lookahead steps: {k}\")\n",
    "        if not lr > 0:\n",
    "            raise ValueError(f\"Invalid Learning Rate: {lr}\")\n",
    "        if not eps > 0:\n",
    "            raise ValueError(f\"Invalid eps: {eps}\")\n",
    "\n",
    "        # prep defaults and init torch.optim base\n",
    "        defaults = dict(\n",
    "            lr=lr,\n",
    "            alpha=alpha,\n",
    "            k=k,\n",
    "            step_counter=0,\n",
    "            betas=betas,\n",
    "            N_sma_threshhold=N_sma_threshhold,\n",
    "            eps=eps,\n",
    "            weight_decay=weight_decay,\n",
    "        )\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "        # adjustable threshold\n",
    "        self.N_sma_threshhold = N_sma_threshhold\n",
    "\n",
    "        # look ahead params\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.k = k\n",
    "\n",
    "        # radam buffer for state\n",
    "        self.radam_buffer = [[None, None, None] for ind in range(10)]\n",
    "\n",
    "        # gc on or off\n",
    "        self.use_gc = use_gc\n",
    "\n",
    "        # level of gradient centralization\n",
    "        self.gc_gradient_threshold = 3 if gc_conv_only else 1\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(Ranger, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        # Evaluate averages and grad, update param tensors\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError(\n",
    "                        \"Ranger optimizer does not support sparse gradients\"\n",
    "                    )\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state[\"step\"] = 0\n",
    "                    state[\"exp_avg\"] = torch.zeros_like(p_data_fp32)\n",
    "                    state[\"exp_avg_sq\"] = torch.zeros_like(p_data_fp32)\n",
    "\n",
    "                    # look ahead weight storage now in state dict\n",
    "                    state[\"slow_buffer\"] = torch.empty_like(p.data)\n",
    "                    state[\"slow_buffer\"].copy_(p.data)\n",
    "\n",
    "                else:\n",
    "                    state[\"exp_avg\"] = state[\"exp_avg\"].type_as(p_data_fp32)\n",
    "                    state[\"exp_avg_sq\"] = state[\"exp_avg_sq\"].type_as(p_data_fp32)\n",
    "\n",
    "                # begin computations\n",
    "                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n",
    "                beta1, beta2 = group[\"betas\"]\n",
    "\n",
    "                # GC operation for Conv layers and FC layers\n",
    "                if grad.dim() > self.gc_gradient_threshold:\n",
    "                    grad.add_(-grad.mean(dim=tuple(range(1, grad.dim())), keepdim=True))\n",
    "\n",
    "                state[\"step\"] += 1\n",
    "\n",
    "                # compute variance mov avg\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                # compute mean moving avg\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "\n",
    "                buffered = self.radam_buffer[int(state[\"step\"] % 10)]\n",
    "\n",
    "                if state[\"step\"] == buffered[0]:\n",
    "                    N_sma, step_size = buffered[1], buffered[2]\n",
    "                else:\n",
    "                    buffered[0] = state[\"step\"]\n",
    "                    beta2_t = beta2 ** state[\"step\"]\n",
    "                    N_sma_max = 2 / (1 - beta2) - 1\n",
    "                    N_sma = N_sma_max - 2 * state[\"step\"] * beta2_t / (1 - beta2_t)\n",
    "                    buffered[1] = N_sma\n",
    "                    if N_sma > self.N_sma_threshhold:\n",
    "                        step_size = math.sqrt(\n",
    "                            (1 - beta2_t)\n",
    "                            * (N_sma - 4)\n",
    "                            / (N_sma_max - 4)\n",
    "                            * (N_sma - 2)\n",
    "                            / N_sma\n",
    "                            * N_sma_max\n",
    "                            / (N_sma_max - 2)\n",
    "                        ) / (1 - beta1 ** state[\"step\"])\n",
    "                    else:\n",
    "                        step_size = 1.0 / (1 - beta1 ** state[\"step\"])\n",
    "                    buffered[2] = step_size\n",
    "\n",
    "                if group[\"weight_decay\"] != 0:\n",
    "                    p_data_fp32.add_(-group[\"weight_decay\"] * group[\"lr\"], p_data_fp32)\n",
    "\n",
    "                # apply lr\n",
    "                if N_sma > self.N_sma_threshhold:\n",
    "                    denom = exp_avg_sq.sqrt().add_(group[\"eps\"])\n",
    "                    p_data_fp32.addcdiv_(-step_size * group[\"lr\"], exp_avg, denom)\n",
    "                else:\n",
    "                    p_data_fp32.add_(-step_size * group[\"lr\"], exp_avg)\n",
    "\n",
    "                p.data.copy_(p_data_fp32)\n",
    "\n",
    "                # integrated look ahead...\n",
    "                # we do it at the param level instead of group level\n",
    "                if state[\"step\"] % group[\"k\"] == 0:\n",
    "                    # get access to slow param tensor\n",
    "                    slow_p = state[\"slow_buffer\"]\n",
    "                    # (fast weights - slow weights) * alpha\n",
    "                    slow_p.add_(self.alpha, p.data - slow_p)\n",
    "                    # copy interpolated weights to RAdam param tensor\n",
    "                    p.data.copy_(slow_p)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8a8f53",
   "metadata": {},
   "source": [
    "## SGDP -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbf291c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"# export\\n@OPTIM_REGISTRY.register()\\nclass SGDP(Optimizer):\\n    \\\"SGDP Optimizer Implementation copied from https://github.com/clovaai/AdamP/blob/master/adamp/sgdp.py\\\"\\n\\n    def __init__(\\n        self,\\n        params: Iterable,\\n        lr=required,\\n        momentum: Union[float, int] = 0,\\n        dampening: Union[float, int] = 0,\\n        weight_decay: Union[float, int] = 0,\\n        nesterov: bool = False,\\n        eps: float = 1e-8,\\n        delta: float = 0.1,\\n        wd_ratio: Union[float, int] = 0.1,\\n    ):\\n\\n        defaults = dict(\\n            lr=lr,\\n            momentum=momentum,\\n            dampening=dampening,\\n            weight_decay=weight_decay,\\n            nesterov=nesterov,\\n            eps=eps,\\n            delta=delta,\\n            wd_ratio=wd_ratio,\\n        )\\n        super(SGDP, self).__init__(params, defaults)\\n\\n    def _channel_view(self, x):\\n        return x.view(x.size(0), -1)\\n\\n    def _layer_view(self, x):\\n        return x.view(1, -1)\\n\\n    def _cosine_similarity(self, x, y, eps, view_func):\\n        x = view_func(x)\\n        y = view_func(y)\\n\\n        return F.cosine_similarity(x, y, dim=1, eps=eps).abs_()\\n\\n    def _projection(self, p, grad, perturb, delta, wd_ratio, eps):\\n        wd = 1\\n        expand_size = [-1] + [1] * (len(p.shape) - 1)\\n        for view_func in [self._channel_view, self._layer_view]:\\n\\n            cosine_sim = self._cosine_similarity(grad, p.data, eps, view_func)\\n\\n            if cosine_sim.max() < delta / math.sqrt(view_func(p.data).size(1)):\\n                p_n = p.data / view_func(p.data).norm(dim=1).view(expand_size).add_(eps)\\n                perturb -= p_n * view_func(p_n * perturb).sum(dim=1).view(expand_size)\\n                wd = wd_ratio\\n\\n                return perturb, wd\\n\\n        return perturb, wd\\n\\n    def step(self, closure=None):\\n        loss = None\\n        if closure is not None:\\n            loss = closure()\\n\\n        for group in self.param_groups:\\n            momentum = group[\\\"momentum\\\"]\\n            dampening = group[\\\"dampening\\\"]\\n            nesterov = group[\\\"nesterov\\\"]\\n\\n            for p in group[\\\"params\\\"]:\\n                if p.grad is None:\\n                    continue\\n                grad = p.grad.data\\n                state = self.state[p]\\n\\n                # State initialization\\n                if len(state) == 0:\\n                    state[\\\"momentum\\\"] = torch.zeros_like(p.data)\\n\\n                # SGD\\n                buf = state[\\\"momentum\\\"]\\n                buf.mul_(momentum).add_(grad, alpha=1 - dampening)\\n                if nesterov:\\n                    d_p = grad + momentum * buf\\n                else:\\n                    d_p = buf\\n\\n                # Projection\\n                wd_ratio = 1\\n                if len(p.shape) > 1:\\n                    d_p, wd_ratio = self._projection(\\n                        p, grad, d_p, group[\\\"delta\\\"], group[\\\"wd_ratio\\\"], group[\\\"eps\\\"]\\n                    )\\n\\n                # Weight decay\\n                if group[\\\"weight_decay\\\"] > 0:\\n                    p.data.mul_(\\n                        1\\n                        - group[\\\"lr\\\"]\\n                        * group[\\\"weight_decay\\\"]\\n                        * wd_ratio\\n                        / (1 - momentum)\\n                    )\\n\\n                # Step\\n                p.data.add_(d_p, alpha=-group[\\\"lr\\\"])\\n\\n        return loss\";\n",
       "                var nbb_formatted_code = \"# export\\n@OPTIM_REGISTRY.register()\\nclass SGDP(Optimizer):\\n    \\\"SGDP Optimizer Implementation copied from https://github.com/clovaai/AdamP/blob/master/adamp/sgdp.py\\\"\\n\\n    def __init__(\\n        self,\\n        params: Iterable,\\n        lr=required,\\n        momentum: Union[float, int] = 0,\\n        dampening: Union[float, int] = 0,\\n        weight_decay: Union[float, int] = 0,\\n        nesterov: bool = False,\\n        eps: float = 1e-8,\\n        delta: float = 0.1,\\n        wd_ratio: Union[float, int] = 0.1,\\n    ):\\n\\n        defaults = dict(\\n            lr=lr,\\n            momentum=momentum,\\n            dampening=dampening,\\n            weight_decay=weight_decay,\\n            nesterov=nesterov,\\n            eps=eps,\\n            delta=delta,\\n            wd_ratio=wd_ratio,\\n        )\\n        super(SGDP, self).__init__(params, defaults)\\n\\n    def _channel_view(self, x):\\n        return x.view(x.size(0), -1)\\n\\n    def _layer_view(self, x):\\n        return x.view(1, -1)\\n\\n    def _cosine_similarity(self, x, y, eps, view_func):\\n        x = view_func(x)\\n        y = view_func(y)\\n\\n        return F.cosine_similarity(x, y, dim=1, eps=eps).abs_()\\n\\n    def _projection(self, p, grad, perturb, delta, wd_ratio, eps):\\n        wd = 1\\n        expand_size = [-1] + [1] * (len(p.shape) - 1)\\n        for view_func in [self._channel_view, self._layer_view]:\\n\\n            cosine_sim = self._cosine_similarity(grad, p.data, eps, view_func)\\n\\n            if cosine_sim.max() < delta / math.sqrt(view_func(p.data).size(1)):\\n                p_n = p.data / view_func(p.data).norm(dim=1).view(expand_size).add_(eps)\\n                perturb -= p_n * view_func(p_n * perturb).sum(dim=1).view(expand_size)\\n                wd = wd_ratio\\n\\n                return perturb, wd\\n\\n        return perturb, wd\\n\\n    def step(self, closure=None):\\n        loss = None\\n        if closure is not None:\\n            loss = closure()\\n\\n        for group in self.param_groups:\\n            momentum = group[\\\"momentum\\\"]\\n            dampening = group[\\\"dampening\\\"]\\n            nesterov = group[\\\"nesterov\\\"]\\n\\n            for p in group[\\\"params\\\"]:\\n                if p.grad is None:\\n                    continue\\n                grad = p.grad.data\\n                state = self.state[p]\\n\\n                # State initialization\\n                if len(state) == 0:\\n                    state[\\\"momentum\\\"] = torch.zeros_like(p.data)\\n\\n                # SGD\\n                buf = state[\\\"momentum\\\"]\\n                buf.mul_(momentum).add_(grad, alpha=1 - dampening)\\n                if nesterov:\\n                    d_p = grad + momentum * buf\\n                else:\\n                    d_p = buf\\n\\n                # Projection\\n                wd_ratio = 1\\n                if len(p.shape) > 1:\\n                    d_p, wd_ratio = self._projection(\\n                        p, grad, d_p, group[\\\"delta\\\"], group[\\\"wd_ratio\\\"], group[\\\"eps\\\"]\\n                    )\\n\\n                # Weight decay\\n                if group[\\\"weight_decay\\\"] > 0:\\n                    p.data.mul_(\\n                        1\\n                        - group[\\\"lr\\\"]\\n                        * group[\\\"weight_decay\\\"]\\n                        * wd_ratio\\n                        / (1 - momentum)\\n                    )\\n\\n                # Step\\n                p.data.add_(d_p, alpha=-group[\\\"lr\\\"])\\n\\n        return loss\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "@OPTIM_REGISTRY.register()\n",
    "class SGDP(Optimizer):\n",
    "    \"SGDP Optimizer Implementation copied from https://github.com/clovaai/AdamP/blob/master/adamp/sgdp.py\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params: Iterable,\n",
    "        lr=required,\n",
    "        momentum: Union[float, int] = 0,\n",
    "        dampening: Union[float, int] = 0,\n",
    "        weight_decay: Union[float, int] = 0,\n",
    "        nesterov: bool = False,\n",
    "        eps: float = 1e-8,\n",
    "        delta: float = 0.1,\n",
    "        wd_ratio: Union[float, int] = 0.1,\n",
    "    ):\n",
    "\n",
    "        defaults = dict(\n",
    "            lr=lr,\n",
    "            momentum=momentum,\n",
    "            dampening=dampening,\n",
    "            weight_decay=weight_decay,\n",
    "            nesterov=nesterov,\n",
    "            eps=eps,\n",
    "            delta=delta,\n",
    "            wd_ratio=wd_ratio,\n",
    "        )\n",
    "        super(SGDP, self).__init__(params, defaults)\n",
    "\n",
    "    def _channel_view(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "    def _layer_view(self, x):\n",
    "        return x.view(1, -1)\n",
    "\n",
    "    def _cosine_similarity(self, x, y, eps, view_func):\n",
    "        x = view_func(x)\n",
    "        y = view_func(y)\n",
    "\n",
    "        return F.cosine_similarity(x, y, dim=1, eps=eps).abs_()\n",
    "\n",
    "    def _projection(self, p, grad, perturb, delta, wd_ratio, eps):\n",
    "        wd = 1\n",
    "        expand_size = [-1] + [1] * (len(p.shape) - 1)\n",
    "        for view_func in [self._channel_view, self._layer_view]:\n",
    "\n",
    "            cosine_sim = self._cosine_similarity(grad, p.data, eps, view_func)\n",
    "\n",
    "            if cosine_sim.max() < delta / math.sqrt(view_func(p.data).size(1)):\n",
    "                p_n = p.data / view_func(p.data).norm(dim=1).view(expand_size).add_(eps)\n",
    "                perturb -= p_n * view_func(p_n * perturb).sum(dim=1).view(expand_size)\n",
    "                wd = wd_ratio\n",
    "\n",
    "                return perturb, wd\n",
    "\n",
    "        return perturb, wd\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            momentum = group[\"momentum\"]\n",
    "            dampening = group[\"dampening\"]\n",
    "            nesterov = group[\"nesterov\"]\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state[\"momentum\"] = torch.zeros_like(p.data)\n",
    "\n",
    "                # SGD\n",
    "                buf = state[\"momentum\"]\n",
    "                buf.mul_(momentum).add_(grad, alpha=1 - dampening)\n",
    "                if nesterov:\n",
    "                    d_p = grad + momentum * buf\n",
    "                else:\n",
    "                    d_p = buf\n",
    "\n",
    "                # Projection\n",
    "                wd_ratio = 1\n",
    "                if len(p.shape) > 1:\n",
    "                    d_p, wd_ratio = self._projection(\n",
    "                        p, grad, d_p, group[\"delta\"], group[\"wd_ratio\"], group[\"eps\"]\n",
    "                    )\n",
    "\n",
    "                # Weight decay\n",
    "                if group[\"weight_decay\"] > 0:\n",
    "                    p.data.mul_(\n",
    "                        1\n",
    "                        - group[\"lr\"]\n",
    "                        * group[\"weight_decay\"]\n",
    "                        * wd_ratio\n",
    "                        / (1 - momentum)\n",
    "                    )\n",
    "\n",
    "                # Step\n",
    "                p.data.add_(d_p, alpha=-group[\"lr\"])\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf47053",
   "metadata": {},
   "source": [
    "## AdamP -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45872f5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"# export\\n@OPTIM_REGISTRY.register()\\nclass AdamP(Optimizer):\\n    \\\"AdamP Optimizer Implementation copied from https://github.com/clovaai/AdamP/blob/master/adamp/adamp.py\\\"\\n\\n    def __init__(\\n        self,\\n        params: Iterable,\\n        lr: Union[float, int] = 1e-3,\\n        betas: Tuple[float, float] = (0.9, 0.999),\\n        eps: float = 1e-8,\\n        weight_decay: Union[float, int] = 0,\\n        delta: float = 0.1,\\n        wd_ratio: float = 0.1,\\n        nesterov: bool = False,\\n    ):\\n\\n        defaults = dict(\\n            lr=lr,\\n            betas=betas,\\n            eps=eps,\\n            weight_decay=weight_decay,\\n            delta=delta,\\n            wd_ratio=wd_ratio,\\n            nesterov=nesterov,\\n        )\\n\\n        super(AdamP, self).__init__(params, defaults)\\n\\n    def _channel_view(self, x):\\n        return x.view(x.size(0), -1)\\n\\n    def _layer_view(self, x):\\n        return x.view(1, -1)\\n\\n    def _cosine_similarity(self, x, y, eps, view_func):\\n        x = view_func(x)\\n        y = view_func(y)\\n\\n        x_norm = x.norm(dim=1).add_(eps)\\n        y_norm = y.norm(dim=1).add_(eps)\\n        dot = (x * y).sum(dim=1)\\n\\n        return dot.abs() / x_norm / y_norm\\n\\n    def _projection(self, p, grad, perturb, delta, wd_ratio, eps):\\n        wd = 1\\n        expand_size = [-1] + [1] * (len(p.shape) - 1)\\n        for view_func in [self._channel_view, self._layer_view]:\\n\\n            cosine_sim = self._cosine_similarity(grad, p.data, eps, view_func)\\n\\n            if cosine_sim.max() < delta / math.sqrt(view_func(p.data).size(1)):\\n                p_n = p.data / view_func(p.data).norm(dim=1).view(expand_size).add_(eps)\\n                perturb -= p_n * view_func(p_n * perturb).sum(dim=1).view(expand_size)\\n                wd = wd_ratio\\n\\n                return perturb, wd\\n\\n        return perturb, wd\\n\\n    def step(self, closure=None):\\n        loss = None\\n        if closure is not None:\\n            loss = closure()\\n\\n        for group in self.param_groups:\\n            for p in group[\\\"params\\\"]:\\n                if p.grad is None:\\n                    continue\\n\\n                grad = p.grad.data\\n                beta1, beta2 = group[\\\"betas\\\"]\\n                nesterov = group[\\\"nesterov\\\"]\\n\\n                state = self.state[p]\\n\\n                # State initialization\\n                if len(state) == 0:\\n                    state[\\\"step\\\"] = 0\\n                    state[\\\"exp_avg\\\"] = torch.zeros_like(p.data)\\n                    state[\\\"exp_avg_sq\\\"] = torch.zeros_like(p.data)\\n\\n                # Adam\\n                exp_avg, exp_avg_sq = state[\\\"exp_avg\\\"], state[\\\"exp_avg_sq\\\"]\\n\\n                state[\\\"step\\\"] += 1\\n                bias_correction1 = 1 - beta1 ** state[\\\"step\\\"]\\n                bias_correction2 = 1 - beta2 ** state[\\\"step\\\"]\\n\\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\\n\\n                denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(\\n                    group[\\\"eps\\\"]\\n                )\\n                step_size = group[\\\"lr\\\"] / bias_correction1\\n\\n                if nesterov:\\n                    perturb = (beta1 * exp_avg + (1 - beta1) * grad) / denom\\n                else:\\n                    perturb = exp_avg / denom\\n\\n                # Projection\\n                wd_ratio = 1\\n                if len(p.shape) > 1:\\n                    perturb, wd_ratio = self._projection(\\n                        p,\\n                        grad,\\n                        perturb,\\n                        group[\\\"delta\\\"],\\n                        group[\\\"wd_ratio\\\"],\\n                        group[\\\"eps\\\"],\\n                    )\\n\\n                # Weight decay\\n                if group[\\\"weight_decay\\\"] > 0:\\n                    p.data.mul_(1 - group[\\\"lr\\\"] * group[\\\"weight_decay\\\"] * wd_ratio)\\n\\n                # Step\\n                p.data.add_(-step_size, perturb)\\n\\n        return loss\";\n",
       "                var nbb_formatted_code = \"# export\\n@OPTIM_REGISTRY.register()\\nclass AdamP(Optimizer):\\n    \\\"AdamP Optimizer Implementation copied from https://github.com/clovaai/AdamP/blob/master/adamp/adamp.py\\\"\\n\\n    def __init__(\\n        self,\\n        params: Iterable,\\n        lr: Union[float, int] = 1e-3,\\n        betas: Tuple[float, float] = (0.9, 0.999),\\n        eps: float = 1e-8,\\n        weight_decay: Union[float, int] = 0,\\n        delta: float = 0.1,\\n        wd_ratio: float = 0.1,\\n        nesterov: bool = False,\\n    ):\\n\\n        defaults = dict(\\n            lr=lr,\\n            betas=betas,\\n            eps=eps,\\n            weight_decay=weight_decay,\\n            delta=delta,\\n            wd_ratio=wd_ratio,\\n            nesterov=nesterov,\\n        )\\n\\n        super(AdamP, self).__init__(params, defaults)\\n\\n    def _channel_view(self, x):\\n        return x.view(x.size(0), -1)\\n\\n    def _layer_view(self, x):\\n        return x.view(1, -1)\\n\\n    def _cosine_similarity(self, x, y, eps, view_func):\\n        x = view_func(x)\\n        y = view_func(y)\\n\\n        x_norm = x.norm(dim=1).add_(eps)\\n        y_norm = y.norm(dim=1).add_(eps)\\n        dot = (x * y).sum(dim=1)\\n\\n        return dot.abs() / x_norm / y_norm\\n\\n    def _projection(self, p, grad, perturb, delta, wd_ratio, eps):\\n        wd = 1\\n        expand_size = [-1] + [1] * (len(p.shape) - 1)\\n        for view_func in [self._channel_view, self._layer_view]:\\n\\n            cosine_sim = self._cosine_similarity(grad, p.data, eps, view_func)\\n\\n            if cosine_sim.max() < delta / math.sqrt(view_func(p.data).size(1)):\\n                p_n = p.data / view_func(p.data).norm(dim=1).view(expand_size).add_(eps)\\n                perturb -= p_n * view_func(p_n * perturb).sum(dim=1).view(expand_size)\\n                wd = wd_ratio\\n\\n                return perturb, wd\\n\\n        return perturb, wd\\n\\n    def step(self, closure=None):\\n        loss = None\\n        if closure is not None:\\n            loss = closure()\\n\\n        for group in self.param_groups:\\n            for p in group[\\\"params\\\"]:\\n                if p.grad is None:\\n                    continue\\n\\n                grad = p.grad.data\\n                beta1, beta2 = group[\\\"betas\\\"]\\n                nesterov = group[\\\"nesterov\\\"]\\n\\n                state = self.state[p]\\n\\n                # State initialization\\n                if len(state) == 0:\\n                    state[\\\"step\\\"] = 0\\n                    state[\\\"exp_avg\\\"] = torch.zeros_like(p.data)\\n                    state[\\\"exp_avg_sq\\\"] = torch.zeros_like(p.data)\\n\\n                # Adam\\n                exp_avg, exp_avg_sq = state[\\\"exp_avg\\\"], state[\\\"exp_avg_sq\\\"]\\n\\n                state[\\\"step\\\"] += 1\\n                bias_correction1 = 1 - beta1 ** state[\\\"step\\\"]\\n                bias_correction2 = 1 - beta2 ** state[\\\"step\\\"]\\n\\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\\n\\n                denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(\\n                    group[\\\"eps\\\"]\\n                )\\n                step_size = group[\\\"lr\\\"] / bias_correction1\\n\\n                if nesterov:\\n                    perturb = (beta1 * exp_avg + (1 - beta1) * grad) / denom\\n                else:\\n                    perturb = exp_avg / denom\\n\\n                # Projection\\n                wd_ratio = 1\\n                if len(p.shape) > 1:\\n                    perturb, wd_ratio = self._projection(\\n                        p,\\n                        grad,\\n                        perturb,\\n                        group[\\\"delta\\\"],\\n                        group[\\\"wd_ratio\\\"],\\n                        group[\\\"eps\\\"],\\n                    )\\n\\n                # Weight decay\\n                if group[\\\"weight_decay\\\"] > 0:\\n                    p.data.mul_(1 - group[\\\"lr\\\"] * group[\\\"weight_decay\\\"] * wd_ratio)\\n\\n                # Step\\n                p.data.add_(-step_size, perturb)\\n\\n        return loss\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "@OPTIM_REGISTRY.register()\n",
    "class AdamP(Optimizer):\n",
    "    \"AdamP Optimizer Implementation copied from https://github.com/clovaai/AdamP/blob/master/adamp/adamp.py\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params: Iterable,\n",
    "        lr: Union[float, int] = 1e-3,\n",
    "        betas: Tuple[float, float] = (0.9, 0.999),\n",
    "        eps: float = 1e-8,\n",
    "        weight_decay: Union[float, int] = 0,\n",
    "        delta: float = 0.1,\n",
    "        wd_ratio: float = 0.1,\n",
    "        nesterov: bool = False,\n",
    "    ):\n",
    "\n",
    "        defaults = dict(\n",
    "            lr=lr,\n",
    "            betas=betas,\n",
    "            eps=eps,\n",
    "            weight_decay=weight_decay,\n",
    "            delta=delta,\n",
    "            wd_ratio=wd_ratio,\n",
    "            nesterov=nesterov,\n",
    "        )\n",
    "\n",
    "        super(AdamP, self).__init__(params, defaults)\n",
    "\n",
    "    def _channel_view(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "    def _layer_view(self, x):\n",
    "        return x.view(1, -1)\n",
    "\n",
    "    def _cosine_similarity(self, x, y, eps, view_func):\n",
    "        x = view_func(x)\n",
    "        y = view_func(y)\n",
    "\n",
    "        x_norm = x.norm(dim=1).add_(eps)\n",
    "        y_norm = y.norm(dim=1).add_(eps)\n",
    "        dot = (x * y).sum(dim=1)\n",
    "\n",
    "        return dot.abs() / x_norm / y_norm\n",
    "\n",
    "    def _projection(self, p, grad, perturb, delta, wd_ratio, eps):\n",
    "        wd = 1\n",
    "        expand_size = [-1] + [1] * (len(p.shape) - 1)\n",
    "        for view_func in [self._channel_view, self._layer_view]:\n",
    "\n",
    "            cosine_sim = self._cosine_similarity(grad, p.data, eps, view_func)\n",
    "\n",
    "            if cosine_sim.max() < delta / math.sqrt(view_func(p.data).size(1)):\n",
    "                p_n = p.data / view_func(p.data).norm(dim=1).view(expand_size).add_(eps)\n",
    "                perturb -= p_n * view_func(p_n * perturb).sum(dim=1).view(expand_size)\n",
    "                wd = wd_ratio\n",
    "\n",
    "                return perturb, wd\n",
    "\n",
    "        return perturb, wd\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                grad = p.grad.data\n",
    "                beta1, beta2 = group[\"betas\"]\n",
    "                nesterov = group[\"nesterov\"]\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state[\"step\"] = 0\n",
    "                    state[\"exp_avg\"] = torch.zeros_like(p.data)\n",
    "                    state[\"exp_avg_sq\"] = torch.zeros_like(p.data)\n",
    "\n",
    "                # Adam\n",
    "                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n",
    "\n",
    "                state[\"step\"] += 1\n",
    "                bias_correction1 = 1 - beta1 ** state[\"step\"]\n",
    "                bias_correction2 = 1 - beta2 ** state[\"step\"]\n",
    "\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "\n",
    "                denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(\n",
    "                    group[\"eps\"]\n",
    "                )\n",
    "                step_size = group[\"lr\"] / bias_correction1\n",
    "\n",
    "                if nesterov:\n",
    "                    perturb = (beta1 * exp_avg + (1 - beta1) * grad) / denom\n",
    "                else:\n",
    "                    perturb = exp_avg / denom\n",
    "\n",
    "                # Projection\n",
    "                wd_ratio = 1\n",
    "                if len(p.shape) > 1:\n",
    "                    perturb, wd_ratio = self._projection(\n",
    "                        p,\n",
    "                        grad,\n",
    "                        perturb,\n",
    "                        group[\"delta\"],\n",
    "                        group[\"wd_ratio\"],\n",
    "                        group[\"eps\"],\n",
    "                    )\n",
    "\n",
    "                # Weight decay\n",
    "                if group[\"weight_decay\"] > 0:\n",
    "                    p.data.mul_(1 - group[\"lr\"] * group[\"weight_decay\"] * wd_ratio)\n",
    "\n",
    "                # Step\n",
    "                p.data.add_(-step_size, perturb)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082fb13e",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d03c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 02_optimizer.ipynb.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_unformatted_code = \"# hide\\nnotebook2script(\\\"02_optimizer.ipynb\\\")\";\n",
       "                var nbb_formatted_code = \"# hide\\nnotebook2script(\\\"02_optimizer.ipynb\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "notebook2script(\"02_optimizer.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04946859",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gale_dev",
   "language": "python",
   "name": "gale_dev"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
